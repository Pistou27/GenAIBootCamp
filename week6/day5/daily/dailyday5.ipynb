{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFQPsMoX6oGz"
      },
      "source": [
        "Ignorer et passer au contenu\n",
        "Historique de chat\n",
        "Vous avez dit :\n",
        "realisons √ßa\n",
        "Image Classification ...\n",
        "\n",
        "Last Updated: July 1st, 2025\n",
        "\n",
        "Daily Challenge: Image Classification with Data Augmentation (Cats vs Dogs)\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "\n",
        "    How to preprocess image data for a convolutional neural network (CNN).\n",
        "    How to apply data augmentation techniques to improve model generalization.\n",
        "    How to build and train a CNN for binary image classification.\n",
        "    How to use dropout to reduce overfitting in a neural network.\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "\n",
        "    A binary image classification model to distinguish between images of cats and dogs.\n",
        "    A visualization of training and validation metrics to analyze model performance.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è Warning ! In this daily challenge, you need to use a VM like DigitalOcean! ‚ö†Ô∏è\n",
        "\n",
        "\n",
        "What You Need to Do\n",
        "\n",
        "1. Preprocess the Data:\n",
        "\n",
        "    Download and extract the Cats vs Dogs dataset here.\n",
        "    Use ImageDataGenerator to rescale and augment the training images (e.g., horizontal flip, rotation, zoom, and shifts).\n",
        "\n",
        "Here is a hint:\n",
        "\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                       directory=train_dir,\n",
        "                                                       shuffle=True,\n",
        "                                                       target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                       class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                          directory=validation_dir,\n",
        "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                          class_mode='binary')\n",
        "\n",
        "\n",
        "    Create separate generators for training and validation data.\n",
        "\n",
        "2. Build the Model:\n",
        "\n",
        "    Create a CNN with:\n",
        "        Three convolutional layers with ReLU activation and max-pooling.\n",
        "        Dropout layers to reduce overfitting.\n",
        "        A fully connected layer with 512 units and ReLU activation.\n",
        "        An output layer with a single unit and sigmoid activation for binary classification.\n",
        "    Compile the model using the Adam optimizer and binary cross-entropy loss.\n",
        "\n",
        "3. Train the Model:\n",
        "\n",
        "    Train the model on the augmented training data for 15 epochs.\n",
        "    Use the validation data to monitor performance during training.\n",
        "\n",
        "4. Evaluate the Model:\n",
        "\n",
        "    Plot the training and validation accuracy and loss to detect overfitting.\n",
        "    Analyze the impact of data augmentation and dropout on model performance.\n",
        "\n",
        "5. Bonus:\n",
        "\n",
        "Retry training the model after Data augmentation: Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples using random transformations that yield believable-looking images. The goal is the model will never see the exact same picture twice during training. This helps expose the model to more aspects of the data and generalize better.\n",
        "There is multiple methods to augment data:\n",
        "\n",
        "    Apply horizontal flip\n",
        "    Randomly rotate the image\n",
        "    Apply zoom augmentation\n",
        "\n",
        "Here the code for Data Augmentation\n",
        "\n",
        "## Apply horizontal flip\n",
        "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "# Found 2000 images belonging to 2 classes.\n",
        "# Take one sample image from the training examples and repeat it five times so that the augmentation is applied to the same image five times\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "\n",
        "\n",
        "## Randomly rotate the image\n",
        "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "\n",
        "# Found 2000 images belonging to 2 classes.\n",
        "\n",
        "\n",
        "## Apply zoom augmentation\n",
        "# zoom_range from 0 - 1 where 1 = 100%.\n",
        "image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "\n",
        "Found 2000 images belonging to 2 classes.\n",
        "\n",
        "#Put it all together\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')\n",
        "Found 2000 images belonging to 2 classes.\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "\n",
        "\n",
        "Create validation data generator\n",
        "\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='binary')\n",
        "Found 1000 images belonging to 2 classes.\n",
        "ChatGPT a dit :\n",
        "\n",
        "Voici une fiche de r√©vision claire et structur√©e sur le th√®me : Image Classification avec Data Augmentation ‚Äì Cats vs Dogs, adapt√©e √† ton niveau et √† ton parcours de d√©veloppeur web en formation IA/ML.\n",
        "üß† Objectif p√©dagogique\n",
        "\n",
        "Tu vas apprendre √† :\n",
        "\n",
        "    Pr√©traiter des images pour un r√©seau de neurones convolutifs (CNN)\n",
        "\n",
        "    Augmenter les donn√©es pour am√©liorer la g√©n√©ralisation du mod√®le\n",
        "\n",
        "    Construire et entra√Æner un CNN pour une classification binaire (chat vs chien)\n",
        "\n",
        "    R√©duire l'overfitting avec des couches de Dropout\n",
        "\n",
        "    Visualiser les performances (accuracy/loss)\n",
        "\n",
        "üß∞ Projet √† r√©aliser\n",
        "\n",
        "Construire un mod√®le de classification d‚Äôimages capable de reconna√Ætre si une image contient un chat ou un chien, en utilisant :\n",
        "\n",
        "    Augmentation de donn√©es (flip, zoom, rotation‚Ä¶)\n",
        "\n",
        "    CNN\n",
        "\n",
        "    Dropout\n",
        "\n",
        "    Visualisation des courbes d'apprentissage\n",
        "\n",
        "‚ö†Ô∏è √Ä ex√©cuter sur une VM avec GPU type DigitalOcean, Colab ou AWS pour √©viter les limites locales.\n",
        "üßº 1. Pr√©traitement et augmentation de donn√©es\n",
        "üì¶ Dataset\n",
        "\n",
        "T√©l√©charge le dataset \"Cats vs Dogs\" (class√© dans deux dossiers cats et dogs).\n",
        "üß™ Pr√©paration des images\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# G√©n√©rateurs simples (normalisation uniquement)\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(\n",
        "    batch_size=batch_size,\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(\n",
        "    batch_size=batch_size,\n",
        "    directory=validation_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "üß™ G√©n√©rateurs avec augmentation de donn√©es\n",
        "\n",
        "image_gen_train = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=45,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.5\n",
        ")\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "    batch_size=batch_size,\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "üß± 2. Construction du mod√®le CNN\n",
        "üìê Architecture recommand√©e :\n",
        "\n",
        "    3 Conv2D + MaxPooling2D\n",
        "\n",
        "    Dropout apr√®s certaines couches\n",
        "\n",
        "    1 couche dense de 512 neurones (activation ReLU)\n",
        "\n",
        "    1 couche de sortie sigmoid (binaire)\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "‚öôÔ∏è Compilation\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "üèãÔ∏è 3. Entra√Ænement du mod√®le\n",
        "\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=train_data_gen.samples // batch_size,\n",
        "    epochs=15,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=val_data_gen.samples // batch_size\n",
        ")\n",
        "\n",
        "üìà 4. √âvaluation & Visualisation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(15)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "üéÅ Bonus : tester diff√©rentes augmentations\n",
        "üîÑ Horizontal flip\n",
        "\n",
        "ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "\n",
        "üîÅ Rotation\n",
        "\n",
        "ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
        "\n",
        "üîç Zoom\n",
        "\n",
        "ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "\n",
        "Je peux te fournir une seconde partie avec les explications th√©oriques (CNN, Dropout, Data Augmentation...), ou t‚Äôaider √† corriger ton code si tu veux tester tout √ßa. Tu veux la suite ?\n",
        "\n",
        "\n",
        "ChatGPT peut commettre des erreurs. Il est recommand√© de v√©rifier les informations importantes. Voir les pr√©f√©rences en mati√®re de cookies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "heQit7X6n7OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d86171-5093-4f5a-9c66-4a5b71fef57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 12:45:51--  https://github.com/devtlv/Datasets-GEN-AI-Bootcamp/raw/refs/heads/main/Week%206/W6D5/Dogs%20vs%20Cats.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/devtlv/Datasets-GEN-AI-Bootcamp/refs/heads/main/Week%206/W6D5/Dogs%20vs%20Cats.zip [following]\n",
            "--2025-07-11 12:45:51--  https://media.githubusercontent.com/media/devtlv/Datasets-GEN-AI-Bootcamp/refs/heads/main/Week%206/W6D5/Dogs%20vs%20Cats.zip\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 856265363 (817M) [application/zip]\n",
            "Saving to: ‚Äòcats_vs_dogs.zip‚Äô\n",
            "\n",
            "cats_vs_dogs.zip    100%[===================>] 816.60M  84.3MB/s    in 4.9s    \n",
            "\n",
            "2025-07-11 12:46:31 (167 MB/s) - ‚Äòcats_vs_dogs.zip‚Äô saved [856265363/856265363]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O cats_vs_dogs.zip \"https://github.com/devtlv/Datasets-GEN-AI-Bootcamp/raw/refs/heads/main/Week%206/W6D5/Dogs%20vs%20Cats.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rb1puSC7NxL"
      },
      "outputs": [],
      "source": [
        "!unzip cats_vs_dogs.zip -d cats_vs_dogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EMoK3Qhp7ag_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wUNvVIkc8H-s"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "train_dir = 'cats_vs_dogs/train'\n",
        "validation_dir = 'cats_vs_dogs/test'\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TKTKq_hA7o-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df1b70a-f9e9-43bb-c68d-db0e78004c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 images belonging to 1 classes.\n",
            "Found 12500 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# G√©n√©rateurs simples (normalisation uniquement)\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(\n",
        "    batch_size=batch_size,\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(\n",
        "    batch_size=batch_size,\n",
        "    directory=validation_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oMeR4FVu8qL0"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential([\n",
        "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pWWE8hYI8ywy"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RvspKw3W81oU"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=train_data_gen.samples // batch_size,\n",
        "    epochs=15,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=val_data_gen.samples // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_loss(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Train Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss over epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qrXtdW0IfoHM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'mae'])"
      ],
      "metadata": {
        "id": "mdeTa4TOgBN1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mae(history):\n",
        "    mae = history.history['mae']\n",
        "    plt.figure()\n",
        "    plt.plot(mae, label='Mean Absolute Error')\n",
        "    plt.title('MAE over epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wyp6L-hjgCza"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}