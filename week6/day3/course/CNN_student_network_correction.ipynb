{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hqnl0AKVXIA4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import skimage.transform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from IPython import display\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kqpBwqTNate"
      },
      "source": [
        "# Tutorial 2a: Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKeYuM-cIXxs"
      },
      "source": [
        "The code below may be helpful in visualizing PyTorch tensors as images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZZd_rI8edYIB"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def show(img):\n",
        "    \"\"\"Show PyTorch tensor img as an image in matplotlib.\"\"\"\n",
        "    npimg = img.cpu().detach().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.grid(False)\n",
        "    plt.gca().axis(\"off\")\n",
        "\n",
        "\n",
        "def display_thumb(img):\n",
        "    display.display(transforms.Resize(128)(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dzfEE578uSNp"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgE0byUgKwM6"
      },
      "source": [
        "Load MNIST and define train/test functions as before. Please make sure you read the code carefully and understand what it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NMUce6PKu-o",
        "outputId": "af67774d-b96e-438a-e6e9-66bb5924e725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "# Load the training and test dataset.\n",
        "mnist_train = datasets.MNIST(\n",
        "    \"/tmp/mnist\", train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "mnist_test = datasets.MNIST(\n",
        "    \"/tmp/mnist\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Size of the batches the data loader will produce.\n",
        "batch_size = 64\n",
        "\n",
        "# This creates the dataloaders.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    mnist_train, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    mnist_test, batch_size=batch_size, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TgAJ94UgK1VU"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, data_loader, optimizer, num_epochs):\n",
        "    \"\"\"Simple training loop for a PyTorch model.\"\"\"\n",
        "\n",
        "    # Make sure model is in training mode.\n",
        "    model.train()\n",
        "\n",
        "    # Move model to the device (CPU or GPU).\n",
        "    model.to(device)\n",
        "\n",
        "    # Exponential moving average of the loss.\n",
        "    ema_loss = None\n",
        "\n",
        "    # Loop over epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Loop over data.\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "\n",
        "            # Forward pass.\n",
        "            output = model(data.to(device))\n",
        "            loss = criterion(output.to(device), target.to(device))\n",
        "\n",
        "            # Backward pass.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # NOTE: It is important to call .item() on the loss before summing.\n",
        "            if ema_loss is None:\n",
        "                ema_loss = loss.item()\n",
        "            else:\n",
        "                ema_loss += (loss.item() - ema_loss) * 0.01\n",
        "\n",
        "        # Print out progress the end of epoch.\n",
        "        print(\n",
        "            \"Train Epoch: {} \\tLoss: {:.6f}\".format(epoch, ema_loss),\n",
        "        )\n",
        "\n",
        "\n",
        "def test(model, data_loader):\n",
        "    \"\"\"Measures the accuracy of a model on a data set.\"\"\"\n",
        "    # Make sure the model is in evaluation mode.\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    # We do not need to maintain intermediate activations while testing.\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Loop over test data.\n",
        "        for data, target in data_loader:\n",
        "\n",
        "            # Forward pass.\n",
        "            output = model(data.to(device))\n",
        "\n",
        "            # Get the label corresponding to the highest predicted probability.\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # Count number of correct predictions.\n",
        "            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Print test accuracy.\n",
        "    percent = 100.0 * correct / len(data_loader.dataset)\n",
        "    print(f\"Accuracy: {correct} / {len(data_loader.dataset)} ({percent:.0f}%)\")\n",
        "    return percent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zz8JUawNath"
      },
      "source": [
        "In the last tutorial, you implemented a naive convolution. In this section you will implement your own version of forward pass of nn.Conv2d without using any of PyTorch's (or numpy's) pre-defined convolutional functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fnlXO_QONati"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_forward_naive(x, w, b, conv_param):\n",
        "    \"\"\"\n",
        "    A naive Python implementation of a convolutional layer.\n",
        "    Inputs:\n",
        "    - x: Input data, torch.Tensor of shape (N, C, H, W)\n",
        "    - w: Filter weights, torch.Tensor of shape (F, C, HH, WW)\n",
        "    - b: Biases, torch.Tensor of shape (F,)\n",
        "    - conv_param: dict with keys:\n",
        "        'stride': int, spacing between receptive fields\n",
        "        'pad':    int, number of zeros to pad each side\n",
        "    Returns:\n",
        "    - out: Output data, torch.Tensor of shape (N, F, H', W')\n",
        "    \"\"\"\n",
        "    N, C, H, W = x.shape\n",
        "    F_filters, _, HH, WW = w.shape\n",
        "    stride, pad = conv_param['stride'], conv_param['pad']\n",
        "\n",
        "    # Compute output dimensions\n",
        "    H_out = (H + 2*pad - HH) // stride + 1\n",
        "    W_out = (W + 2*pad - WW) // stride + 1\n",
        "\n",
        "    # Zero-pad input symmetrically on H and W axes\n",
        "    # F.pad takes (pad_left, pad_right, pad_top, pad_bottom)\n",
        "    x_pad = F.pad(x, (pad, pad, pad, pad))  # :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "    # Initialize output tensor\n",
        "    out = torch.zeros((N, F_filters, H_out, W_out), dtype=x.dtype, device=x.device)\n",
        "\n",
        "    # Iterate over each image in batch, each filter, and each spatial position\n",
        "    for n in range(N):\n",
        "        for f in range(F_filters):\n",
        "            for i in range(H_out):\n",
        "                for j in range(W_out):\n",
        "                    h_start = i * stride\n",
        "                    h_end   = h_start + HH\n",
        "                    w_start = j * stride\n",
        "                    w_end   = w_start + WW\n",
        "\n",
        "                    # Elementwise product and sum, then add bias\n",
        "                    window = x_pad[n, :, h_start:h_end, w_start:w_end]\n",
        "                    out[n, f, i, j] = torch.sum(window * w[f]) + b[f]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3AILT-TNati"
      },
      "source": [
        "You can test your implementation by running the following testing code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg5n1iAuNati",
        "outputId": "a6be6ed5-8f33-42b7-bd89-4cda6c467f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing conv_forward_naive\n",
            "difference:  tensor(4.1913e-08)\n",
            "Nice work! Your implementation of a convolution layer works correctly.\n"
          ]
        }
      ],
      "source": [
        "# Make convolution module.\n",
        "w_shape = (3, 3, 4, 4)\n",
        "w = torch.linspace(-0.2, 0.3, steps=torch.prod(torch.tensor(w_shape))).reshape(w_shape)\n",
        "b = torch.linspace(-0.1, 0.2, steps=3)\n",
        "\n",
        "# Compute output of module and compare against reference values.\n",
        "x_shape = (2, 3, 4, 4)\n",
        "x = torch.linspace(-0.1, 0.5, steps=torch.prod(torch.tensor(x_shape))).reshape(x_shape)\n",
        "out = conv_forward_naive(x, w, b, {\"stride\": 2, \"pad\": 1})\n",
        "\n",
        "correct_out = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [[-0.08759809, -0.10987781], [-0.18387192, -0.2109216]],\n",
        "            [[0.21027089, 0.21661097], [0.22847626, 0.23004637]],\n",
        "            [[0.50813986, 0.54309974], [0.64082444, 0.67101435]],\n",
        "        ],\n",
        "        [\n",
        "            [[-0.98053589, -1.03143541], [-1.19128892, -1.24695841]],\n",
        "            [[0.69108355, 0.66880383], [0.59480972, 0.56776003]],\n",
        "            [[2.36270298, 2.36904306], [2.38090835, 2.38247847]],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compare your output to ours; difference should be around e-8\n",
        "print(\"Testing conv_forward_naive\")\n",
        "rel_error = ((out - correct_out) / (out + correct_out + 1e-6)).mean()\n",
        "print(\"difference: \", rel_error)\n",
        "if abs(rel_error) < 1e-6:\n",
        "    print(\"Nice work! Your implementation of a convolution layer works correctly.\")\n",
        "else:\n",
        "    print(\n",
        "        \"Something is wrong. The output was expected to be {} but it was {}\".format(\n",
        "            correct_out, out\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjTq5-k_q8s_"
      },
      "source": [
        "\n",
        "We will now replace the logistic regressor from the last tutorial by a small convolutional network with two convolutional layers and a linear layer, and ReLU activations in between the layers. Implement the model and use the same functions as before to train and test the convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t4hWoUYpp05",
        "outputId": "59d33b50-0369-4977-e8f2-ada618688ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 \tLoss: 0.200040\n",
            "Train Epoch: 1 \tLoss: 0.116278\n",
            "Train Epoch: 2 \tLoss: 0.091611\n",
            "Train Epoch: 3 \tLoss: 0.070258\n",
            "Train Epoch: 4 \tLoss: 0.066025\n",
            "Accuracy: 9800 / 10000 (98%)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "98.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "    \"\"\"Simple convolutional network.\"\"\"\n",
        "\n",
        "    def __init__(self, image_side_size, num_classes, in_channels=1):\n",
        "        super().__init__()\n",
        "        # Convolutional block: output channels = 16, kernel 3×3, padding=1, stride=1\n",
        "        # followed by ReLU and 2×2 max pooling.\n",
        "        self.conv_network = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # After pooling, spatial dims halve: from image_side_size → image_side_size/2\n",
        "        flattened_size = 16 * (image_side_size // 2) * (image_side_size // 2)\n",
        "        self.linear = nn.Linear(flattened_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_network(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# Create and train convolutional network.\n",
        "# The accuracy should be around 96%.\n",
        "conv_model = ConvolutionalNetwork(28, 10)\n",
        "###########################################################################\n",
        "# TODO: Create criterion and optimize here.                               #\n",
        "###########################################################################\n",
        "# Classification loss for C-way logits\n",
        "criterion = nn.CrossEntropyLoss()  # :contentReference[oaicite:4]{index=4}\n",
        "\n",
        "# Stochastic gradient descent with learning rate 0.01 and momentum 0.9\n",
        "optimizer = torch.optim.SGD(conv_model.parameters(), lr=0.01, momentum=0.9)  # :contentReference[oaicite:5]{index=5}\n",
        "\n",
        "\n",
        "train(conv_model, criterion, train_loader, optimizer, num_epochs=5)\n",
        "test(conv_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1stmqcKlNatj"
      },
      "source": [
        "Inspect the filters in the first layer of the trained convolutional network. What do they look like? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "LuSuQHT6bNdU",
        "outputId": "66ed6a18-c604-4711-ba17-759aa75561f3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "first_conv = list(conv_model.conv_network.children())[0]\n",
        "show(\n",
        "    torchvision.utils.make_grid(\n",
        "        first_conv.weight,\n",
        "        normalize=True,\n",
        "        nrow=8,\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
