{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d459d29b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Understanding LLM Evaluation:\n",
    "\n",
    "    Explain why evaluating LLMs is more complex than traditional software.\n",
    "    _____________\n",
    "    Identify key reasons for evaluating an LLM’s safety.\n",
    "    _____________\n",
    "    Describe how adversarial testing contributes to LLM improvement.\n",
    "    _____________\n",
    "    Discuss the limitations of automated evaluation metrics and how they compare to human evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9163a",
   "metadata": {},
   "source": [
    "Sortie non déterministe : Contrairement aux logiciels traditionnels, les LLM peuvent produire différentes réponses pour la même entrée.\n",
    "\n",
    "Multiplicité des réponses possibles : Il n’y a souvent pas une seule \"bonne\" réponse, mais plusieurs formulations valides.\n",
    "\n",
    "______________________\n",
    "\n",
    "Prévention des abus : Éviter que le modèle soit utilisé pour générer du contenu malveillant (désinformation, discours haineux, harcèlement, etc.).\n",
    "\n",
    "Protection des utilisateurs : Éviter que le modèle donne des conseils dangereux ou des informations sensibles.\n",
    "\n",
    "Alignement éthique : S’assurer que le modèle agit selon des principes éthiques et réglementaires (ex. RGPD, biais sociaux).\n",
    "\n",
    "______________________\n",
    "\n",
    "Détecter ses failles : Identifier les cas où il donne des réponses erronées, biaisées ou dangereuses.\n",
    "\n",
    "Renforcer sa robustesse : En corrigeant les erreurs révélées par ces tests, on entraîne des modèles plus solides.\n",
    "\n",
    "______________________\n",
    "\n",
    "Limites des métriques automatiques :\n",
    "\n",
    "    Manque de compréhension du contexte : Elles évaluent souvent mot à mot (BLEU, ROUGE) sans comprendre le sens.\n",
    "\n",
    "    Insensibilité à la fluidité ou à la tonalité : Elles ne jugent pas si une réponse est naturelle ou appropriée.\n",
    "\n",
    "    Incapacité à évaluer la factualité ou l’éthique : Elles ne détectent pas les erreurs factuelles ou les réponses biaisées.\n",
    "\n",
    "    Réduction à des scores numériques : Elles donnent un chiffre qui ne reflète pas la richesse ou la qualité réelle d'une réponse.\n",
    "\n",
    "Comparaison avec l’évaluation humaine :\n",
    "\n",
    "    L’évaluation humaine est plus fiable, car elle tient compte du sens, du contexte, de l’utilité et de l’acceptabilité de la réponse.\n",
    "\n",
    "    Mais elle est coûteuse et lente, ce qui la rend difficile à appliquer à grande échelle.\n",
    "\n",
    "    La solution idéale est hybride : Utiliser des métriques automatiques pour des pré-filtrages rapides, puis compléter par des évaluations humaines ciblées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24271184",
   "metadata": {},
   "source": [
    "Applying BLEU and ROUGE Metrics:\n",
    "\n",
    "    Calculate the BLEU score for the following example:\n",
    "        Reference: “Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.”\n",
    "        Generated: “Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”\n",
    "\n",
    "    Calculate the ROUGE score for the following example:\n",
    "        Reference: “In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.”\n",
    "        Generated: “To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.”\n",
    "\n",
    "    Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.\n",
    "\n",
    "    Suggest improvements or alternative methods for evaluating text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe78206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log, exp\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f04c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Simple tokenizer: lowercase and split on spaces\n",
    "    return text.lower().replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return Counter([' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "\n",
    "def modified_precision(hypothesis_tokens, reference_tokens, n):\n",
    "    hyp_ngrams = get_ngrams(hypothesis_tokens, n)\n",
    "    ref_ngrams = get_ngrams(reference_tokens, n)\n",
    "    \n",
    "    overlap = hyp_ngrams & ref_ngrams\n",
    "    match_count = sum(overlap.values())\n",
    "    total_count = max(sum(hyp_ngrams.values()), 1)\n",
    "    \n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    return match_count / total_count\n",
    "\n",
    "def brevity_penalty(hypothesis_tokens, reference_tokens):\n",
    "    c = len(hypothesis_tokens)\n",
    "    r = len(reference_tokens)\n",
    "    if c > r:\n",
    "        return 1\n",
    "    else:\n",
    "        return exp(1 - r / c)\n",
    "\n",
    "def compute_bleu(hypothesis, reference):\n",
    "    hypothesis_tokens = tokenize(hypothesis)\n",
    "    reference_tokens = tokenize(reference)\n",
    "    \n",
    "    precisions = []\n",
    "    for n in range(1, 5):\n",
    "        p_n = modified_precision(hypothesis_tokens, reference_tokens, n)\n",
    "        # smoothing: if p_n == 0, use small epsilon\n",
    "        if p_n == 0:\n",
    "            p_n = 1e-8\n",
    "        precisions.append(p_n)\n",
    "    \n",
    "    # Geometric mean of modified precisions\n",
    "    log_precisions = [log(p) for p in precisions]\n",
    "    geo_mean = exp(sum(log_precisions) / 4)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = brevity_penalty(hypothesis_tokens, reference_tokens)\n",
    "    \n",
    "    bleu = bp * geo_mean\n",
    "    return bleu, precisions, bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d40433da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.002\n",
      "Precisions (p1 to p4): [0.333, 0.176, 0.062, 0.0]\n",
      "Brevity Penalty: 0.895\n"
     ]
    }
   ],
   "source": [
    "# ==== Example ====\n",
    "reference = \"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"\n",
    "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"\n",
    "\n",
    "bleu, p_ns, bp = compute_bleu(generated, reference)\n",
    "\n",
    "print(f\"BLEU score: {bleu:.3f}\")\n",
    "print(f\"Precisions (p1 to p4): {[round(p, 3) for p in p_ns]}\")\n",
    "print(f\"Brevity Penalty: {bp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7429685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.stats import fisher_exact\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0779\n"
     ]
    }
   ],
   "source": [
    "#Autre tests\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Assure-toi que les ressources nécessaires sont téléchargées\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Définition des phrases\n",
    "reference = \"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"\n",
    "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"\n",
    "\n",
    "# Tokenisation simple\n",
    "from nltk.tokenize import word_tokenize\n",
    "ref_tokens = word_tokenize(reference)\n",
    "gen_tokens = word_tokenize(generated)\n",
    "\n",
    "# BLEU score avec lissage\n",
    "chencherry = SmoothingFunction()\n",
    "score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=chencherry.method1)\n",
    "\n",
    "print(f\"BLEU score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "112d6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\alexa\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexa\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=cf21a529d4d545b2072880ea616fdd1dbde8885e12f71dc3655ea9403fff9aa5\n",
      "  Stored in directory: c:\\users\\alexa\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ROUGE1 -> Precision: 0.471, Recall: 0.333, F1: 0.390\n",
      "ROUGE2 -> Precision: 0.188, Recall: 0.130, F1: 0.154\n",
      "ROUGEL -> Precision: 0.353, Recall: 0.250, F1: 0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge-score\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = (\"In the face of rapid climate change, global initiatives must focus on reducing \"\n",
    "             \"carbon emissions and developing sustainable energy sources to mitigate environmental impact.\")\n",
    "\n",
    "generated = (\"To counteract climate change, worldwide efforts should aim to lower carbon emissions \"\n",
    "             \"and enhance renewable energy development.\")\n",
    "\n",
    "# Initialize scorer for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Score\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "# Print nicely\n",
    "for key in scores:\n",
    "    precision = scores[key].precision\n",
    "    recall = scores[key].recall\n",
    "    f1 = scores[key].fmeasure\n",
    "    print(f\"{key.upper()} -> Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f19310",
   "metadata": {},
   "source": [
    "Les métriques BLEU (traduction automatique) et ROUGE (résumé automatique) sont très utilisées dans l’évaluation de textes générés automatiquement. Cependant, elles ont de sérieuses limites lorsqu’on les applique à des textes créatifs ou sensibles au contexte, comme les dialogues, résumés complexes, ou textes littéraires.\n",
    "\n",
    "C'est une comparaison purement lexical.\n",
    "Aucune compréhension du sens sémantique et peu de correlation avec les jugement humains.\n",
    "Dépendance à une ou plusieurs références.*\n",
    "En Rouge, ordre des mot peu pris en compte.\n",
    "\n",
    " Alternatives et améliorations proposées\n",
    "\n",
    "Métriques sémantiques à base d’embeddings\n",
    "Évaluation avec un modèle de langage (LLM-as-a-judge)\n",
    "Évaluation humaine (la référence absolue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20797a3b",
   "metadata": {},
   "source": [
    "Model A: perplexity = 1/0.8 = 1.25\n",
    "Model B: perplexity = 1/0.4 = 2.50\n",
    "\n",
    "Model A > Model B en terme de perplixity (meilleur accurance)\n",
    "\n",
    "Que signifie une perplexité égale à 100 ?\n",
    "\n",
    "    Une perplexité de 100 indique que le modèle est aussi incertain que s’il devait choisir entre 100 mots possibles, tous équiprobables.\n",
    "\n",
    "    Le modèle est donc assez peu performant, il prédit mal le mot suivant.\n",
    "\n",
    "Pour améliorer, augmenter le nombre de données, faire du fine tunning, changer les hyperparamètres, de meilleurs données etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d3b88",
   "metadata": {},
   "source": [
    "“Apologies, but comprehend I do not. Could you rephrase your question?”\n",
    "C'est du vieux anglais donc (2/5) car gramaticalement correct.\n",
    "\n",
    "MAIS EN LANGUAGE YODA: (4/5) car normalement c'est “Apologies, I offer. Comprehend, I do not. Rephrase your question, could you?”\n",
    "\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⠤⠐⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡌⡦⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⣼⡊⢀⠔⠀⠀⣄⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣄⣀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣶⠃⠉⠡⡠⠤⠊⠀⠠⣀⣀⡠⠔⠒⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⢟⠿⠛⠛⠁\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡇⠀⠀⠀⠀⠑⠶⠖⠊⠁⠀⠀⠀⡀⠀⠀⠀⢀⣠⣤⣤⡀⠀⠀⠀⠀⠀⢀⣠⣤⣶⣿⣿⠟⡱⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣾⣿⡇⠀⢀⡠⠀⠀⠀⠈⠑⢦⣄⣀⣀⣽⣦⣤⣾⣿⠿⠿⠿⣿⡆⠀⠀⢀⠺⣿⣿⣿⣿⡿⠁⡰⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣧⣠⠊⣠⣶⣾⣿⣿⣶⣶⣿⣿⠿⠛⢿⣿⣫⢕⡠⢥⣈⠀⠙⠀⠰⣷⣿⣿⣿⡿⠋⢀⠜⠁⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢿⣿⣿⣿⣿⣰⣿⣿⠿⣛⡛⢛⣿⣿⣟⢅⠀⠀⢿⣿⠕⢺⣿⡇⠩⠓⠂⢀⠛⠛⠋⢁⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠘⢶⡶⢶⣶⣦⣤⣤⣤⣤⣤⣀⣀⣀⣀⡀⠀⠘⣿⣿⣿⠟⠁⡡⣒⣬⢭⢠⠝⢿⡡⠂⠀⠈⠻⣯⣖⣒⣺⡭⠂⢀⠈⣶⣶⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠙⠳⣌⡛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣵⣨⣿⣿⡏⢀⠪⠎⠙⠿⣋⠴⡃⢸⣷⣤⣶⡾⠋⠈⠻⣶⣶⣶⣷⣶⣷⣿⣟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠈⠛⢦⣌⡙⠛⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠩⠭⡭⠴⠊⢀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⣿⣿⡇⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠈⠙⠓⠦⣄⡉⠛⠛⠻⢿⣿⣿⣿⣷⡀⠀⠀⠀⠀⢀⣰⠋⠀⠀⠀⠀⠀⣀⣰⠤⣳⣿⣿⣿⣿⣟⠑⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠒⠒⠶⢺⣿⣿⣿⣿⣦⣄⣀⣴⣿⣯⣤⣔⠒⠚⣒⣉⣉⣴⣾⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠹⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣉⣉⣤⣿⣿⣿⣿⣿⣿⡿⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠟⡁⡆⠙⢶⣀⠀⢀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣴⣶⣾⣿⣟⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢛⣩⣴⣿⠇⡇⠸⡆⠙⢷⣄⠻⣿⣦⡄⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣎⢻⣿⣿⣿⣿⣿⣿⣿⣭⣭⣭⣵⣶⣾⣿⣿⣿⠟⢰⢣⠀⠈⠀⠀⠙⢷⡎⠙⣿⣦⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⡟⣿⡆⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠟⠛⠋⠁⢀⠇⢸⡇⠀⠀⠀⠀⠈⠁⠀⢸⣿⡆⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡜⡿⡘⣿⣿⣿⣿⣿⣶⣶⣤⣤⣤⣤⣤⣤⣤⣴⡎⠖⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⣿⣷⡄⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⡀⠘⢿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠛⠋⡟⠀⠀⣸⣷⣀⣤⣀⣀⣀⣤⣤⣾⣿⣿⣿⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣓⡲⠬⢭⣙⡛⠿⣿⣿⣶⣦⣀⠀⡜⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣛⣓⠶⠦⠥⣀⠙⠋⠉⠉⠻⣄⣀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣆⠐⣦⣠⣷⠊⠁⠀⠀⡭⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢉⣛⡛⢻⡗⠂⠀⢀⣷⣄⠈⢆⠉⠙⠻⢿⣿⣿⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠘⣿⣿⡟⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⣉⢁⣴⣿⣿⣿⣾⡇⢀⣀⣼⡿⣿⣷⡌⢻⣦⡀⠀⠈⠙⠛⠿⠏⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⡄⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠛⠛⢯⡉⠉⠉⠉⠉⠛⢼⣿⠿⠿⠦⡙⣿⡆⢹⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⠄⠈⠻⠿⠿⠿⠿⠿⠿⠛⠛⠿⠛⠉⠁⠀⠀⠀⠀⠀⠀⠻⠿⠿⠿⠿⠟⠉⠀⠀⠤⠴⠶⠌⠿⠘⠿⠿⠿⠿⠶⠤⠀⠀⠀⠀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25c666",
   "metadata": {},
   "source": [
    "Capital et non Capitol\n",
    "\n",
    "Normalisation et correction orthographique automatique \n",
    "\n",
    "Compréhension contextuelle renforcée : Entraîner le modèle à repérer des erreurs fréquentes et à demander une clarification quand c'est ambigu\n",
    "\n",
    "Demander une précision à l'utilisateur si nécessaire.\n",
    "\n",
    "\n",
    "Pour les phrases qui testent la robustesse du LLM, il y'a : \n",
    "\n",
    "Who received the last Nobel Prize in Mathematics? -> Le Nobel n'existe pas\n",
    "\n",
    "Which game received the Ace of Gold at Cannes in 2027? -> Données futurs\n",
    "\n",
    "What is a \"grolaxyis\"? -> mot inventé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf71bc",
   "metadata": {},
   "source": [
    "| Métrique               | Principe                                                                                                                            | Points forts                                                                               | Limites                                                                                                 |\n",
    "| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- |\n",
    "| **ROUGE**              | Mesure le recouvrement des n-grammes (unigrammes, bigrammes, etc.) entre résumé généré et résumé de référence                       | Simple, rapide, standard pour résumé, corrèle assez bien avec qualité de résumé            | Sensible aux reformulations et paraphrases, pénalise la diversité lexicale                              |\n",
    "| **BLEU**               | Mesure la précision des n-grammes du texte généré par rapport à une ou plusieurs références (principalement utilisée en traduction) | Utile pour la traduction, simple, standardisée                                             | Moins adaptée au résumé car favorise la correspondance exacte, pénalise reformulations et condensations |\n",
    "| **BERTScore**          | Utilise des embeddings contextuels (BERT) pour mesurer la similarité sémantique entre résumé généré et référence                    | Capte la similarité sémantique même sans correspondance exacte, robuste aux reformulations | Coûteux en calcul, dépend du modèle utilisé, plus difficile à interpréter                               |\n",
    "| **Évaluation humaine** | Jugement subjectif de la qualité par des annotateurs (fluency, fidélité, cohérence)                                                 | Plus fiable et complète, évalue la cohérence globale, la pertinence contextuelle           | Coûteuse, longue à mettre en place, variabilité inter-annotateurs                                       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
